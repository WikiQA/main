{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2023 CITS4012 Assignment\n",
        "*Make sure you change the file name with your student id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "\n",
        "Cetain libraries needs to be installed for the whole code to work. Please make sure to run the below cmd functions. before running the full code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.5.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.1.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (62.1.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Installing spacy for Named Entity Tagging\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "# Authenticate and create the PyDrive client.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# To Tabulate the values\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.5.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.3)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (62.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Downloading the pre-trained NLP Model for Named Entity Tagging\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Libararies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/naufaln/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "import re\n",
        "import math\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from statistics import median\n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# For Named Enity Tagging\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "# importing necessary libraries for TF-IDF\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# For Modelling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Training and Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importing Training and Testing Data\n",
        "training_data = pd.read_csv('./Data/WikiQA-train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('./Data/WikiQA-test.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['QuestionID', 'Question', 'DocumentID', 'DocumentTitle', 'SentenceID',\n",
            "       'Sentence', 'Label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(training_data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Function to convert the given data to required format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shrink_columns(df):\n",
        "    # Create a new dataframe with four columns\n",
        "    new_df = pd.DataFrame(columns=['QuestionID', 'Question', 'Document', 'Answer'])\n",
        "\n",
        "    # Loop through the unique QuestionIDs in the original dataframe\n",
        "    for qid in df['QuestionID'].unique():\n",
        "        # Get the first question associated with this QuestionID\n",
        "        first_question = df.loc[df['QuestionID'] == qid, 'Question'].iloc[0]\n",
        "        \n",
        "        # Get all sentences associated with this QuestionID\n",
        "        sentences = df.loc[df['QuestionID'] == qid, 'Sentence']\n",
        "        \n",
        "        # Concatenate all sentences into a single string\n",
        "        concatenated_sentence = ' '.join(sentences)\n",
        "        \n",
        "        # Get the sentence associated with this QuestionID where the Label is 1\n",
        "        answer = df.loc[(df['QuestionID'] == qid) & (df['Label'] == 1), 'Sentence']\n",
        "        \n",
        "        # If there is at least one such row, get the first sentence\n",
        "        if not answer.empty:\n",
        "            answer = answer.iloc[0]\n",
        "        else:\n",
        "            answer = ''\n",
        "        \n",
        "        # Add the QuestionID, first_question, concatenated_sentence, and answer to the new dataframe\n",
        "        new_row = {'QuestionID': qid, 'Question': first_question, 'Document': concatenated_sentence, 'Answer': answer}\n",
        "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>QuestionID</th>\n",
              "      <th>Question</th>\n",
              "      <th>Document</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q0</td>\n",
              "      <td>HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US</td>\n",
              "      <td>African immigration to the United States refer...</td>\n",
              "      <td>As such, African immigrants are to be distingu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q3</td>\n",
              "      <td>how large were early jails</td>\n",
              "      <td>A prison (from Old French prisoun), also known...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q4</td>\n",
              "      <td>how a water pump works</td>\n",
              "      <td>A small, electrically powered pump A large, el...</td>\n",
              "      <td>Pumps operate by some mechanism (typically rec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q20</td>\n",
              "      <td>how old was sue lyon when she made lolita</td>\n",
              "      <td>Lolita is a 1962 comedy-drama film by Stanley ...</td>\n",
              "      <td>The actress who played Lolita, Sue Lyon , was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q33</td>\n",
              "      <td>how are antibodies used in</td>\n",
              "      <td>Each antibody binds to a specific antigen ; an...</td>\n",
              "      <td>An antibody (Ab), also known as an immunoglobu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Q47</td>\n",
              "      <td>how old is alicia in 2009</td>\n",
              "      <td>Alicia Augello Cook (born January 25, 1981), k...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Q49</td>\n",
              "      <td>how big is a land section generally</td>\n",
              "      <td>Sectioning a township . Perfectly square full ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Q54</td>\n",
              "      <td>how old was shakespeare's juliet</td>\n",
              "      <td>Juliet is one of the title characters in Willi...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Q57</td>\n",
              "      <td>how are fire bricks made</td>\n",
              "      <td>Refractory bricks in a torpedo car used for ha...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Q59</td>\n",
              "      <td>HOW MUCH IS CENTAVOS IN MEXICO</td>\n",
              "      <td>The peso ( sign : $; code : MXN) is the curren...</td>\n",
              "      <td>The peso is subdivided into 100 centavos, repr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Q64</td>\n",
              "      <td>How long was Mickie James with WWE?</td>\n",
              "      <td>Mickie Laree James (born August 31, 1979) is a...</td>\n",
              "      <td>James appeared in World Wrestling Entertainmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Q68</td>\n",
              "      <td>how can something taste good to one person</td>\n",
              "      <td>Taste bud Taste, gustatory perception, or gust...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Q69</td>\n",
              "      <td>How did Bob Marley die?</td>\n",
              "      <td>Nesta Robert \"Bob\" Marley, OM (6 February 1945...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Q72</td>\n",
              "      <td>how much was the production of willy wonka and...</td>\n",
              "      <td>Willy Wonka &amp; the Chocolate Factory is a 1971 ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Q79</td>\n",
              "      <td>how much is a ream of paper</td>\n",
              "      <td>Various measures of paper quantity have been a...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Q81</td>\n",
              "      <td>how can i open a usda slaughterhouse</td>\n",
              "      <td>Workers and cattle in a slaughterhouse A slaug...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   QuestionID                                           Question  \\\n",
              "0          Q0    HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n",
              "1          Q3                         how large were early jails   \n",
              "2          Q4                             how a water pump works   \n",
              "3         Q20          how old was sue lyon when she made lolita   \n",
              "4         Q33                         how are antibodies used in   \n",
              "5         Q47                          how old is alicia in 2009   \n",
              "6         Q49                how big is a land section generally   \n",
              "7         Q54                   how old was shakespeare's juliet   \n",
              "8         Q57                           how are fire bricks made   \n",
              "9         Q59                     HOW MUCH IS CENTAVOS IN MEXICO   \n",
              "10        Q64                How long was Mickie James with WWE?   \n",
              "11        Q68         how can something taste good to one person   \n",
              "12        Q69                            How did Bob Marley die?   \n",
              "13        Q72  how much was the production of willy wonka and...   \n",
              "14        Q79                        how much is a ream of paper   \n",
              "15        Q81               how can i open a usda slaughterhouse   \n",
              "\n",
              "                                             Document  \\\n",
              "0   African immigration to the United States refer...   \n",
              "1   A prison (from Old French prisoun), also known...   \n",
              "2   A small, electrically powered pump A large, el...   \n",
              "3   Lolita is a 1962 comedy-drama film by Stanley ...   \n",
              "4   Each antibody binds to a specific antigen ; an...   \n",
              "5   Alicia Augello Cook (born January 25, 1981), k...   \n",
              "6   Sectioning a township . Perfectly square full ...   \n",
              "7   Juliet is one of the title characters in Willi...   \n",
              "8   Refractory bricks in a torpedo car used for ha...   \n",
              "9   The peso ( sign : $; code : MXN) is the curren...   \n",
              "10  Mickie Laree James (born August 31, 1979) is a...   \n",
              "11  Taste bud Taste, gustatory perception, or gust...   \n",
              "12  Nesta Robert \"Bob\" Marley, OM (6 February 1945...   \n",
              "13  Willy Wonka & the Chocolate Factory is a 1971 ...   \n",
              "14  Various measures of paper quantity have been a...   \n",
              "15  Workers and cattle in a slaughterhouse A slaug...   \n",
              "\n",
              "                                               Answer  \n",
              "0   As such, African immigrants are to be distingu...  \n",
              "1                                                      \n",
              "2   Pumps operate by some mechanism (typically rec...  \n",
              "3   The actress who played Lolita, Sue Lyon , was ...  \n",
              "4   An antibody (Ab), also known as an immunoglobu...  \n",
              "5                                                      \n",
              "6                                                      \n",
              "7                                                      \n",
              "8                                                      \n",
              "9   The peso is subdivided into 100 centavos, repr...  \n",
              "10  James appeared in World Wrestling Entertainmen...  \n",
              "11                                                     \n",
              "12                                                     \n",
              "13                                                     \n",
              "14                                                     \n",
              "15                                                     "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shrinked_training_data = shrink_columns(training_data)\n",
        "shrinked_test_data = shrink_columns(test_data)\n",
        "\n",
        "# Print the new dataframe\n",
        "shrinked_test_data.head(16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "shrinked_training_data['labels'] = ''\n",
        "\n",
        "for i in range(len(shrinked_training_data)):\n",
        "    paragraph = shrinked_training_data.loc[i, 'Document']\n",
        "    token = [None for i in range(len(paragraph))]\n",
        "    if shrinked_training_data.loc[i, 'Answer'] != '':\n",
        "        start_index = paragraph.find(shrinked_training_data.loc[i, 'Answer'])\n",
        "        end_index = start_index + len(shrinked_training_data.loc[i, 'Answer'])\n",
        "        token[start_index] = 'S'\n",
        "        for j in range(start_index+1, end_index):\n",
        "            token[j] = 'I'\n",
        "        token[end_index-1] = 'E'\n",
        "    shrinked_training_data.at[i, 'labels'] = token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.QA Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(sentance):\n",
        "    sent_text=[]\n",
        "    content_text = re.sub(r'\\([^)]*\\)', '', sentance.lower())\n",
        "    sent_text.extend(word_tokenize(content_text))\n",
        "\n",
        "\n",
        "    #commenting it in doubt whether to remove punctuations or not\n",
        "    '''\n",
        "    # Removing punctuation and changing all characters to lower case\n",
        "    normalized_text = []\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9.]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    \n",
        "    print(sent_text)\n",
        "    '''\n",
        "    return sent_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tokenising Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_list(sequences):\n",
        "    tokenized_list = []\n",
        "    for seq in sequences:\n",
        "        tokenized_list.append(tokenize(seq))\n",
        "\n",
        "    return tokenized_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word_embedding(tokens):\n",
        "    word_embed_model = api.load(\"glove-twitter-25\")\n",
        "    # Create the Embedding lookup table for the first pre-trained embedding\n",
        "    emb_dim = word_embed_model.vector_size\n",
        "\n",
        "    emb_table = []\n",
        "    for i, word in enumerate(tokens):\n",
        "        if str(word) in word_embed_model:\n",
        "            emb_table.append(word_embed_model[str(word)])\n",
        "        else:\n",
        "            emb_table.append([0]*emb_dim)\n",
        "    emb_table = np.array(emb_table)\n",
        "        \n",
        "    return emb_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Average Length of Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_average_length(sequences):\n",
        "    list_of_lengths = []\n",
        "    avg_length = 0\n",
        "    for seq in sequences:\n",
        "        list_of_lengths.append(len(seq))\n",
        "    \n",
        "    avg_length = round(sum(list_of_lengths)/len(list_of_lengths))\n",
        "    return avg_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Padding Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences(sequences):\n",
        "    # Find the max length of the sequences\n",
        "    max_length = max(len(seq) for seq in sequences)\n",
        "    \n",
        "    # Pad the sequences based on the max length\n",
        "    padded_sequences = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        num_padding = max_length - len(seq)\n",
        "        padded_seq = seq + ['[PAD]'] * num_padding\n",
        "        padded_sequences.append(padded_seq)\n",
        "    \n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the list of questions\n",
        "qn_list = shrinked_training_data['Question']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Getting full list of words in Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_Q_full_word_list(sentance_list):\n",
        "    Q_full_word_list = list()\n",
        "    for Q in sentance_list:\n",
        "        Q = Q.lower()\n",
        "        Q_word_list = tokenize(Q)\n",
        "        Q_full_word_list.append(Q_word_list)\n",
        "    return(Q_full_word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word to Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q_full_word_list = get_Q_full_word_list(qn_list)\n",
        "\n",
        "# set up a vocab to index dictionary\n",
        "Q_word_to_ix = {\"<BOS>\": 0, \"<EOS>\":1} \n",
        "for sentence in Q_full_word_list:\n",
        "    for word in sentence:\n",
        "        if word not in Q_word_to_ix:\n",
        "            Q_word_to_ix[word] = len(Q_word_to_ix)\n",
        "word_list = list(Q_word_to_ix.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Padding the Questions with Max length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenised_qn_list = tokenize_list(qn_list)\n",
        "padded_qn_list = pad_sequences(tokenised_qn_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Words        0         1         2         3         4        5  \\\n",
            "0          how  0.50363  0.257190 -0.160620 -0.065956 -0.300250 -0.04945   \n",
            "1          are  0.18660 -0.098326 -0.122680 -0.938220 -0.401610  0.63830   \n",
            "2          the -1.22180 -1.043200 -0.096553 -0.041382 -1.586400 -0.16366   \n",
            "3   directions -0.70318 -1.149900  0.181350 -0.075280 -1.242100  0.15105   \n",
            "4           of -0.57660 -0.007180 -0.543770 -0.792860 -0.259320 -0.82164   \n",
            "5          the  1.10400 -0.346290  0.088792 -0.255400 -0.023462  0.51487   \n",
            "6     velocity  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "7          and  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "8        force  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "9      vectors  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "10     related  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "11          in  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "12           a  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "13    circular  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "14      motion  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "15       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "16       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "17       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "18       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "19       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "20       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "21       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "22       [PAD]  0.00000  0.000000  0.000000  0.000000  0.000000  0.00000   \n",
            "\n",
            "           6        7        8  ...        15        16        17       18  \\\n",
            "0   1.840800 -0.51396 -0.48495  ... -0.382700  0.558990 -0.377980  0.21429   \n",
            "1   1.668600 -0.68036 -0.98359  ...  0.113770 -0.527250 -0.793120  0.58203   \n",
            "2   0.153580 -1.13710  1.10920  ... -0.485680  0.390000 -0.001951 -0.10218   \n",
            "3  -0.044360 -1.33870  1.14690  ...  0.714770  0.061513 -0.838420  0.30405   \n",
            "4  -0.021945 -1.51590 -0.68476  ... -0.241650  0.503420  0.653250 -0.20674   \n",
            "5   0.749100  1.78580  0.16928  ... -0.485680  0.390000 -0.001951 -0.10218   \n",
            "6   0.000000  0.00000  0.00000  ... -0.079302 -0.248830  0.874790  0.97253   \n",
            "7   0.000000  0.00000  0.00000  ... -0.151730  0.203830 -0.774960  0.17629   \n",
            "8   0.000000  0.00000  0.00000  ...  1.096500 -0.268220  0.168490 -0.42866   \n",
            "9   0.000000  0.00000  0.00000  ... -0.580330  0.839790 -1.011200  1.25590   \n",
            "10  0.000000  0.00000  0.00000  ...  0.732610  0.158990  0.112690  0.45328   \n",
            "11  0.000000  0.00000  0.00000  ...  0.364940 -0.004254  0.966870 -1.56740   \n",
            "12  0.000000  0.00000  0.00000  ...  0.155500 -0.551810  0.346710 -0.57379   \n",
            "13  0.000000  0.00000  0.00000  ...  0.487210  0.038217  1.106300  1.03770   \n",
            "14  0.000000  0.00000  0.00000  ...  1.022300 -0.238800  0.613470  0.20442   \n",
            "15  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "16  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "17  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "18  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "19  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "20  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "21  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "22  0.000000  0.00000  0.00000  ...  0.000000  0.000000  0.000000  0.00000   \n",
            "\n",
            "          19        20        21        22        23        24  \n",
            "0  -0.416130  0.536260  0.145020 -0.379150  0.204890 -0.756330  \n",
            "1  -0.618290  0.370250  0.226100 -0.730140 -0.101900 -0.213820  \n",
            "2   0.212620 -0.861460  0.172630  0.187830 -0.842500 -0.312080  \n",
            "3  -0.151050  0.250530 -1.251600  0.365330 -0.790040 -1.191300  \n",
            "4   0.276390 -0.790970  0.104320 -0.617500 -0.545920 -0.069893  \n",
            "5   0.212620 -0.861460  0.172630  0.187830 -0.842500 -0.312080  \n",
            "6   0.502900 -0.835860 -0.016799  1.115400 -0.090956 -0.706490  \n",
            "7  -0.108840 -0.312340  0.240100 -0.360970 -0.049996 -0.724700  \n",
            "8   0.112500  0.216300 -0.148930  0.648060 -1.103200 -0.237760  \n",
            "9   0.940870 -1.530400 -0.372370 -0.559450  0.030520  0.140280  \n",
            "10 -0.240210  0.044432 -0.056222 -0.922030 -0.371470 -0.529590  \n",
            "11 -0.404540 -0.795570 -0.005054  0.021972 -0.736380  0.652770  \n",
            "12 -0.307170  0.043623 -0.397070  0.645510 -0.335370  0.020467  \n",
            "13 -0.082312 -0.532050 -0.852490  0.541240 -0.525090 -0.316730  \n",
            "14 -0.287230 -0.764330 -0.467140  0.690760 -0.317930 -0.209840  \n",
            "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "22  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[23 rows x 51 columns]\n"
          ]
        }
      ],
      "source": [
        "input_questions = pd.DataFrame()\n",
        "for i in range(2):\n",
        "    input_questions[\"Words\"] = padded_qn_list[i]\n",
        "    \n",
        "    # Word Embedding\n",
        "    # generating word embedding values \n",
        "    qn_emb_table = word_embedding(padded_qn_list[i])\n",
        "    qn_emb_table = pd.DataFrame(qn_emb_table)\n",
        "    input_questions = pd.concat([input_questions, qn_emb_table], axis=1)\n",
        "\n",
        "print(input_questions)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_labels(document, answer):\n",
        "    labels = [\"Not Answer\" for i in range(len(document))]\n",
        "    if answer != '':\n",
        "        #start_index = document.index(answer)\n",
        "        start_index = [i for i in range(len(document)-len(answer)+1) if document[i:i+len(answer)] == answer]\n",
        "        print(start_index)\n",
        "        start_index = start_index[0]\n",
        "        end_index = start_index + len(answer)\n",
        "        labels[start_index] = 'Start'\n",
        "        for j in range(start_index+1, end_index):\n",
        "            labels[j] = 'Middle'\n",
        "        labels[end_index-1] = 'End'\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_idf(tokens):\n",
        "    tf_idf_list = list()\n",
        "    tknzr = TreebankWordTokenizer()\n",
        "    DF = {}\n",
        "\n",
        "    # get each unique word in the doc - and count the number of occurrences in the document\n",
        "    for term in np.unique(tokens):\n",
        "        try:\n",
        "            DF[term] +=1\n",
        "        except:\n",
        "            DF[term] =1\n",
        "\n",
        "    tf_idf = []\n",
        "    N = len(tokens) \n",
        "    doc_id = 0\n",
        "    counter = Counter(tokens)\n",
        "    total_num_words = len(tokens) \n",
        "    for term in tokens:\n",
        "        tf = counter[term]/total_num_words\n",
        "        df = DF[term]\n",
        "        idf = math.log(N/(df+1))+1\n",
        "        tf_idf.append(tf*idf)\n",
        "\n",
        "    doc_id += 1\n",
        "    tf_idf_list.append(tf_idf)\n",
        "\n",
        "    return tf_idf_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pos_tagging(tokens):\n",
        "    tagged_words = pos_tag(tokens)\n",
        "    tagged_words_list, tags_list = zip(*tagged_words)\n",
        "    return tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Named Entity Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [],
      "source": [
        "def NE_Tagging(document):\n",
        "    NE_Tag_table = []\n",
        "    tokens = []\n",
        "    # loading pre-trained model of NER\n",
        "    entity_tagging_model = en_core_web_sm.load()\n",
        "    article = entity_tagging_model(document)\n",
        "    sentences = [x for x in article.sents]\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            NE_Tag_table.append(str(word.ent_type_))\n",
        "            tokens.append(str(word).lower())\n",
        "    for i in range(len(NE_Tag_table)):\n",
        "        if(NE_Tag_table[i] == ''):\n",
        "            NE_Tag_table[i] = \"O\"\n",
        "\n",
        "    return tokens, NE_Tag_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to get the wordnet POS tag and convert to use with lemmatizer\n",
        "def get_wordnet_pos(tags):\n",
        "    if tags.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tags.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tags.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif tags.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatization(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmitized = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token,tag in zip(tokens['Words'], tokens['POS Tags']) ]  \n",
        "    return lemmitized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_list = shrinked_training_data['Document']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Delete later if not needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "def question_padding(questions):\n",
        "    # Tokenize the questions\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(questions)\n",
        "    question_tokenised = tokenizer.texts_to_sequences(questions)\n",
        "\n",
        "    # Get the maximum length of all the questions in the 'Question' column of the dataframe\n",
        "    max_length = max(len(question) for question in question_tokenised)\n",
        "\n",
        "    # Pad the sequence based on the max length defined, to make sure the list of sequences are of same length\n",
        "    def pad_sequence(seq_list, max_length, index_dict):\n",
        "        res = []\n",
        "        for seq in seq_list:\n",
        "            temp = seq[:]\n",
        "            # Pad the sequence with zeros if its length is less than max_length\n",
        "            temp += [index_dict['[PAD]']] * (max_length - len(seq))\n",
        "            res.append(temp)\n",
        "        return np.array(res)\n",
        "\n",
        "    # Add the Question_padded column with the padded sequences\n",
        "    word2index = tokenizer.word_index\n",
        "    word2index['[PAD]'] = 0 # Add the '[PAD]' token to the word2index dictionary\n",
        "    questions_padded = pad_sequence(question_tokenised, max_length, word2index)\n",
        "    print(tabulate(questions_padded, headers='firstrow', tablefmt='fancy_grid'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25]\n",
            "       Words      Labels POS Tags Named Entity Tags    TF-IDF         0  \\\n",
            "0          a  Not Answer       DT                 O  0.279108  0.212940   \n",
            "1     partly  Not Answer       RB                 O  0.069777 -0.980010   \n",
            "2   submerge  Not Answer      VBN                 O  0.069777 -1.125800   \n",
            "3    glacier  Not Answer       NN                 O  0.418663 -1.221800   \n",
            "4       cave  Not Answer       NN                 O  0.488440 -1.287700   \n",
            "..       ...         ...      ...               ...       ...       ...   \n",
            "59      year  Not Answer       NN              DATE  0.069777 -0.049064   \n",
            "60         -  Not Answer        :              DATE  0.069777  0.771700   \n",
            "61     round  Not Answer       NN              DATE  0.069777 -0.746810   \n",
            "62       ice  Not Answer       NN                 O  0.348886 -0.768120   \n",
            "63         .  Not Answer        .                 O  0.209331  0.695860   \n",
            "\n",
            "          1         2         3         4  ...        15       16       17  \\\n",
            "0   0.31035  0.176940  0.874980  0.067926  ...  0.155500 -0.55181  0.34671   \n",
            "1  -0.25930 -0.885720 -1.151100 -0.991740  ... -1.042200  0.28337  1.62540   \n",
            "2   0.31330 -0.553400  0.838480 -1.005400  ...  1.411800  0.10928  0.17972   \n",
            "3  -1.04320 -0.096553 -0.041382 -1.586400  ... -0.351020  0.82824  1.06300   \n",
            "4  -0.77947  0.286900  0.506440 -0.709260  ...  0.650120  0.53041  0.30999   \n",
            "..      ...       ...       ...       ...  ...       ...      ...      ...   \n",
            "59  0.68725  0.671870 -1.037300  0.096446  ... -0.737380  0.45681 -0.94921   \n",
            "60 -1.06020 -0.343830 -0.092640  0.031247  ... -1.535000 -0.55627  0.64587   \n",
            "61  0.40575  0.762440 -0.499260 -0.067388  ... -0.006814  0.21504 -0.74715   \n",
            "62 -0.27978  0.603820  0.569370 -0.573220  ... -0.077737  1.04180 -0.83902   \n",
            "63 -1.14690 -0.417970 -0.022311 -0.023801  ... -0.570580 -0.50861 -0.16575   \n",
            "\n",
            "          18        19        20        21        22        23        24  \n",
            "0  -0.573790 -0.307170  0.043623 -0.397070  0.645510 -0.335370  0.020467  \n",
            "1   0.352770 -0.456990 -0.003474 -0.818420  0.718170  0.888630 -0.776020  \n",
            "2   1.086300 -0.161710 -0.830280 -0.421170  0.084073 -0.907580 -0.698360  \n",
            "3   0.751630  0.461950 -0.713030  0.049465  0.752360 -1.297100  0.630070  \n",
            "4  -0.112240  0.411900 -0.209590  0.038357  0.274850 -0.872900  0.553230  \n",
            "..       ...       ...       ...       ...       ...       ...       ...  \n",
            "59 -0.409110 -0.067633 -0.796040  0.202000 -0.358740  0.133020 -0.268090  \n",
            "60 -0.724300 -0.399000 -0.311720 -0.588340 -0.110270 -0.067876  1.072300  \n",
            "61 -0.626570  0.195420 -1.023900  0.339970  1.563000 -0.336330  0.046848  \n",
            "62 -0.052313  0.593330 -1.159700  0.353680  0.415390  0.498750  0.895480  \n",
            "63 -0.981530 -0.821300  0.243330 -0.144820 -0.678770  0.706100  0.408330  \n",
            "\n",
            "[64 rows x 30 columns]\n"
          ]
        }
      ],
      "source": [
        "D_full_word_list = list()\n",
        "for i in range(1):\n",
        "    doc = shrinked_training_data.loc[i, \"Document\"]\n",
        "\n",
        "    answer = tokenize(shrinked_training_data.loc[i, 'Answer'])\n",
        "\n",
        "    doc_tokens = tokenize(doc)\n",
        "\n",
        "    word_features = pd.DataFrame()\n",
        "\n",
        "    # 1. Words\n",
        "    word_list, NE_Tag_list = NE_Tagging(doc)\n",
        "    word_features[\"Words\"] = word_list\n",
        "\n",
        "    # 2. Labels\n",
        "    # generating Labels to identify the position of the Answer in the document\n",
        "    labels = generate_labels(word_list, answer)\n",
        "    word_features[\"Labels\"] = labels\n",
        "\n",
        "    # Creating a dictionary of Words in Documents\n",
        "    D_full_word_list.append(word_list)\n",
        "\n",
        "    # 3. POS tagging\n",
        "    pos_tags = pos_tagging(word_list)\n",
        "    word_features[\"POS Tags\"] = pos_tags\n",
        "\n",
        "    # 4. Lemmetization\n",
        "    word_list = lemmatization(word_features)\n",
        "    word_features[\"Words\"] = word_list\n",
        "    \n",
        "    # 5. Named Entity Tags\n",
        "    word_features[\"Named Entity Tags\"] = NE_Tag_list\n",
        "    \n",
        "    # 7. TF_IDF\n",
        "    tf_idf_list = tf_idf(word_list)\n",
        "    word_features[\"TF-IDF\"] = tf_idf_list\n",
        "\n",
        "    # 8. Word Embedding\n",
        "    # generating word embedding values \n",
        "    emb_table = word_embedding(word_list)\n",
        "    \n",
        "    emb_table = pd.DataFrame(emb_table)\n",
        "\n",
        "    word_features = pd.concat([word_features, emb_table], axis=1)\n",
        "\n",
        "    print(word_features)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Question Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Question Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Model Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNGfO0h9I3W"
      },
      "source": [
        "###3.1. Input Embedding Ablation Study\n",
        "\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVsyvrc9VHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7nFwMo9WBE"
      },
      "source": [
        "###3.2. Attention Ablation Study\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRK-BeiNSVi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VAR8GF9hSD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzGjUe6NDnB"
      },
      "source": [
        "###3.3. Hyper Parameter Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xj4PNyrNDBH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
