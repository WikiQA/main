{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2023 CITS4012 Assignment\n",
        "*Make sure you change the file name with your student id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.7)\n",
            "Requirement already satisfied: click in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (8.1.2)\n",
            "Requirement already satisfied: joblib in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (2022.7.9)\n"
          ]
        }
      ],
      "source": [
        "# Installing spacy for nltk\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.5.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (62.1.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Collecting typing-extensions>=4.2.0\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4\n",
            "    Uninstalling typing-extensions-3.7.4:\n",
            "      Successfully uninstalled typing-extensions-3.7.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
          ]
        }
      ],
      "source": [
        "# Installing spacy for Named Entity Tagging\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (0.9.0)\n"
          ]
        }
      ],
      "source": [
        "# To Tabulate the values\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To overrie the error while installing en_core_web_sm\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (62.1.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Downloading the pre-trained NLP Model for Named Entity Tagging\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint\n",
        "import re\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from statistics import median\n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# For Named Enity Tagging\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "# importing necessary libraries for TF-IDF\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# For Modelling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# importing Training and Testing Data\n",
        "training_data = pd.read_csv('./Data/WikiQA-train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('./Data/WikiQA-test.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Formatting the Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shrink_columns(df):\n",
        "    # Create a new dataframe with four columns\n",
        "    new_df = pd.DataFrame(columns=['QuestionID', 'Question', 'Document', 'Answer'])\n",
        "\n",
        "    # Loop through the unique QuestionIDs in the original dataframe\n",
        "    for qid in df['QuestionID'].unique():\n",
        "        # Get the first question associated with this QuestionID\n",
        "        first_question = df.loc[df['QuestionID'] == qid, 'Question'].iloc[0]\n",
        "        \n",
        "        # Get all sentences associated with this QuestionID\n",
        "        sentences = df.loc[df['QuestionID'] == qid, 'Sentence']\n",
        "        \n",
        "        # Concatenate all sentences into a single string\n",
        "        concatenated_sentence = ' '.join(sentences)\n",
        "        \n",
        "        # Get the sentence associated with this QuestionID where the Label is 1\n",
        "        answer = df.loc[(df['QuestionID'] == qid) & (df['Label'] == 1), 'Sentence']\n",
        "        \n",
        "        if not answer.empty:\n",
        "            answer = answer.iloc[0]\n",
        "        else:\n",
        "            answer = None\n",
        "        \n",
        "        # Add the QuestionID, first_question, concatenated_sentence, and answer to the new dataframe\n",
        "        new_row = {'QuestionID': qid, 'Question': first_question, 'Document': concatenated_sentence, 'Answer': answer}\n",
        "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "formatted_training_data = shrink_columns(training_data)\n",
        "formatted_test_data = shrink_columns(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for Labelling the document tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generateLabels(document, answer):\n",
        "    labels = [\"[Not Answer]\" for i in range(len(document))]\n",
        "    if(answer != ''):\n",
        "        start_index = document.find(answer)\n",
        "        end_index = start_index + len(answer)\n",
        "        for j in range(start_index, end_index):\n",
        "            labels[j] = '[Answer]'\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for tokenising a sentance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(sentance):\n",
        "    sent_text=[]\n",
        "    content_text = re.sub(r'\\([^)]*\\)', '', sentance.lower())\n",
        "    sent_text.extend(word_tokenize(content_text))\n",
        "\n",
        "\n",
        "    #commenting it in doubt whether to remove punctuations or not\n",
        "    \n",
        "    # Removing punctuation and changing all characters to lower case\n",
        "    normalized_text = []\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9.]+\", \"\", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    \n",
        "    return normalized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for Tokenising a list of sentances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenizeList(sequences):\n",
        "    tokenized_list = []\n",
        "    for seq in sequences:\n",
        "        tokenized_list.append(tokenize(seq))\n",
        "\n",
        "    return tokenized_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for word embedding a sentance (Using Word2Vec - Skip Gram Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2Vec(sentance):\n",
        "    # Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "    wv_sg_model = Word2Vec(sentences=sentance, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "    word_2_vec = []\n",
        "    for word in sentance:\n",
        "        word_2_vec = word_2_vec.append(wv_sg_model.wv[word])\n",
        "    return word_2_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for word embedding a document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2VecDocuments(document):\n",
        "    word_2_vec = []\n",
        "    for sentance in document:\n",
        "        word_2_vec = word_2_vec.append(word2Vec(sentance))\n",
        "    return word_2_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get the average length of a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getAverageLength(sequences):\n",
        "    list_of_lengths = []\n",
        "    avg_length = 0\n",
        "    for seq in sequences:\n",
        "        list_of_lengths.append(len(seq))\n",
        "    \n",
        "    avg_length = round(sum(list_of_lengths)/len(list_of_lengths))\n",
        "    return avg_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to add padding to the sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences(sequences):\n",
        "    # Find the max length of the sequences\n",
        "    max_length = round( max(len(seq) for seq in sequences))\n",
        "    print(max_length)\n",
        "    \n",
        "    # Pad the sequences based on the max length\n",
        "    padded_sequences = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        num_padding = max_length - len(seq)\n",
        "        padded_seq = seq + ['[PAD]'] * num_padding\n",
        "        padded_sequences.append(padded_seq)\n",
        "    \n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to find the TF-IDF values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tfIdf(tokens):\n",
        "    tf_idf_list = list()\n",
        "    DF = {}\n",
        "\n",
        "    # get each unique word in the doc - and count the number of occurrences in the document\n",
        "    for term in np.unique(tokens):\n",
        "        try:\n",
        "            DF[term] +=1\n",
        "        except:\n",
        "            DF[term] =1\n",
        "\n",
        "    tf_idf = []\n",
        "    N = len(tokens) \n",
        "    doc_id = 0\n",
        "    counter = Counter(tokens)\n",
        "    total_num_words = len(tokens) \n",
        "    for term in tokens:\n",
        "        tf = counter[term]/total_num_words\n",
        "        df = DF[term]\n",
        "        idf = math.log(N/(df+1))+1\n",
        "        tf_idf.append(tf*idf)\n",
        "\n",
        "    doc_id += 1\n",
        "    tf_idf_list.append(tf_idf)\n",
        "\n",
        "    return tf_idf_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def posTagging(tokens):\n",
        "    tagged_words = pos_tag(tokens)\n",
        "    tagged_words_list, tags_list = zip(*tagged_words)\n",
        "    return tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to find the Named Entity Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nerTagging(document):\n",
        "    NE_Tag_table = []\n",
        "    tokens = []\n",
        "    # loading pre-trained model of NER\n",
        "    entity_tagging_model = en_core_web_sm.load()\n",
        "    article = entity_tagging_model(document)\n",
        "    sentences = [x for x in article.sents]\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            NE_Tag_table.append(str(word.ent_type_))\n",
        "            tokens.append(str(word).lower())\n",
        "    for i in range(len(NE_Tag_table)):\n",
        "        if(NE_Tag_table[i] == ''):\n",
        "            NE_Tag_table[i] = \"O\"\n",
        "\n",
        "    return tokens, NE_Tag_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get the wordnet POS tag and convert to use with lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getWordnetPos(tags):\n",
        "    if tags.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tags.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tags.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif tags.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to Lemmattize the words using the POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatization(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmitized = [lemmatizer.lemmatize(token, pos=getWordnetPos(tag)) for token,tag in zip(tokens['Words'], tokens['POS Tags']) ]  \n",
        "    return lemmitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# To be Removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = formatted_training_data[11:16]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.QA Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Model Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNGfO0h9I3W"
      },
      "source": [
        "###3.1. Input Embedding Ablation Study\n",
        "\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVsyvrc9VHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7nFwMo9WBE"
      },
      "source": [
        "###3.2. Attention Ablation Study\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRK-BeiNSVi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VAR8GF9hSD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzGjUe6NDnB"
      },
      "source": [
        "###3.3. Hyper Parameter Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xj4PNyrNDBH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
