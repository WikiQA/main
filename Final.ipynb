{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2023 CITS4012 Assignment\n",
        "*Make sure you change the file name with your student id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (8.1.2)\n",
            "Requirement already satisfied: joblib in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from nltk) (2022.7.9)\n"
          ]
        }
      ],
      "source": [
        "# Installing spacy for nltk\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.22.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (62.1.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (3.1.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Installing spacy for Named Entity Tagging\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (0.9.0)\n"
          ]
        }
      ],
      "source": [
        "# To Tabulate the values\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To overrie the error while installing en_core_web_sm\n",
        "import os\n",
        "\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (62.1.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/naufaln/opt/miniconda3/envs/cits5508-2022/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Downloading the pre-trained NLP Model for Named Entity Tagging\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/naufaln/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from statistics import median\n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# For Named Enity Tagging\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "# importing necessary libraries for TF-IDF\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# For Modelling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# importing Training and Testing Data\n",
        "training_data = pd.read_csv('./Data/WikiQA-train.tsv', sep='\\t')\n",
        "test_data = pd.read_csv('./Data/WikiQA-test.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Formatting the Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shrink_columns(df):\n",
        "    # Create a new dataframe with four columns\n",
        "    new_df = pd.DataFrame(columns=['QuestionID', 'Question', 'Document', 'Answer'])\n",
        "\n",
        "    # Loop through the unique QuestionIDs in the original dataframe\n",
        "    for qid in df['QuestionID'].unique():\n",
        "        # Get the first question associated with this QuestionID\n",
        "        first_question = df.loc[df['QuestionID'] == qid, 'Question'].iloc[0]\n",
        "        \n",
        "        # Get all sentences associated with this QuestionID\n",
        "        sentences = df.loc[df['QuestionID'] == qid, 'Sentence']\n",
        "        \n",
        "        # Concatenate all sentences into a single string\n",
        "        concatenated_sentence = ' '.join(sentences)\n",
        "        \n",
        "        # Get the sentence associated with this QuestionID where the Label is 1\n",
        "        answer = df.loc[(df['QuestionID'] == qid) & (df['Label'] == 1), 'Sentence']\n",
        "        \n",
        "        if not answer.empty:\n",
        "            answer = answer.iloc[0]\n",
        "        else:\n",
        "            answer = \"\"\n",
        "        \n",
        "        # Add the QuestionID, first_question, concatenated_sentence, and answer to the new dataframe\n",
        "        new_row = {'QuestionID': qid, 'Question': first_question, 'Document': concatenated_sentence, 'Answer': answer}\n",
        "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "formatted_training_data = shrink_columns(training_data)\n",
        "formatted_test_data = shrink_columns(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for Labelling the document tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generateLabels(padded_document, answer, len_org_document):\n",
        "    labels = [\"[Not Answer]\" for i in range(len(padded_document))]\n",
        "    if answer != \"\":\n",
        "        start_index = [i for i in range(len(padded_document)-len(answer)+1) if padded_document[i:i+len(answer)] == answer]\n",
        "        if start_index:\n",
        "            start_index = start_index[0]\n",
        "            end_index = start_index + len(answer)\n",
        "            labels[start_index] = '[Answer]'\n",
        "            for j in range(start_index+1, end_index):\n",
        "                labels[j] = '[Answer]'\n",
        "            labels[end_index-1] = '[Answer]'\n",
        "    # labelling the padding\n",
        "    for i in range(len_org_document, len(padded_document)):\n",
        "        labels[i] = \"[Pad]\"\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def removePunctuations(sentence):\n",
        "    words = sentence.split()\n",
        "    formatted_sentance = []\n",
        "    for word in words:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", '', word.lower())\n",
        "        formatted_sentance.append(tokens)\n",
        "    return formatted_sentance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for tokenising a sentance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(sentance):\n",
        "    sent_text=[]\n",
        "    normalized_text = []\n",
        "    sent_text.extend(word_tokenize(sentance))\n",
        "    \n",
        "    # Removing punctuation and changing all characters to lower case\n",
        "\n",
        "    for string in sent_text:\n",
        "        tokens = re.sub(r\"[^a-z0-9.]+\", '', string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "\n",
        "    final_text = []\n",
        "    for text in normalized_text:\n",
        "        if text != '':\n",
        "            final_text.append(text)\n",
        "\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for Tokenising a list of sentances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenizeList(sequences):\n",
        "    tokenized_list = []\n",
        "    for seq in sequences:\n",
        "        tokenized_list.append(tokenize(seq))\n",
        "\n",
        "    return tokenized_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for word embedding a sentance (Using Word2Vec - Skip Gram Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2Vec(sentance):\n",
        "    # Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "    wv_sg_model = Word2Vec([sentance], vector_size=50, window=3, min_count=1, workers=2, sg=1)\n",
        "\n",
        "    word_2_vec = list()\n",
        "    for word in sentance:\n",
        "        word_2_vec.append(wv_sg_model.wv[word])\n",
        "    return word_2_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function for word embedding a document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word2VecDocuments(document):\n",
        "    word_2_vec = list()\n",
        "    for sentance in document:\n",
        "        word_2_vec.append(word2Vec(sentance))\n",
        "    return word_2_vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get the average length of a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getAverageLength(sequences):\n",
        "    list_of_lengths = list()\n",
        "    avg_length = 0\n",
        "    for seq in sequences:\n",
        "        list_of_lengths.append(len(seq))\n",
        "    \n",
        "    avg_length = round(sum(list_of_lengths)/len(list_of_lengths))\n",
        "    return avg_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to add padding to the sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sequences(sequences):\n",
        "    # Find the max length of the sequences\n",
        "    max_length = round( max(len(seq) for seq in sequences))\n",
        "    \n",
        "    # Pad the sequences based on the max length\n",
        "    padded_sequences = list()\n",
        "\n",
        "    for seq in sequences:\n",
        "        num_padding = max_length - len(seq)\n",
        "        padded_seq = seq + ['[PAD]'] * num_padding\n",
        "        padded_sequences.append(padded_seq)\n",
        "    \n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to find the TF-IDF values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tfIdf(tokens, org_len):\n",
        "    tf_idf_list = list()\n",
        "    DF = {}\n",
        "\n",
        "    # get each unique word in the doc - and count the number of occurrences in the document\n",
        "    for term in np.unique(tokens):\n",
        "        try:\n",
        "            DF[term] +=1\n",
        "        except:\n",
        "            DF[term] =1\n",
        "\n",
        "    tf_idf = []\n",
        "    N = len(tokens) \n",
        "    doc_id = 0\n",
        "    counter = Counter(tokens)\n",
        "    total_num_words = len(tokens) \n",
        "    for term in tokens[0:org_len]:\n",
        "        tf = counter[term]/total_num_words\n",
        "        df = DF[term]\n",
        "        idf = math.log(N/(df+1))+1\n",
        "        tf_idf.append(tf*idf)\n",
        "    for term in range(org_len,len(tokens)):\n",
        "        tf_idf.append(0)\n",
        "\n",
        "    doc_id += 1\n",
        "    tf_idf_list.append(tf_idf)\n",
        "\n",
        "    return tf_idf_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def posTagging(tokens, len_org_doc):\n",
        "    tagged_words = pos_tag(tokens[0:len_org_doc])\n",
        "    tagged_words_list, tags_list = zip(*tagged_words)\n",
        "    tags_list = list(tags_list)\n",
        "    for i in range(len_org_doc,len(tokens)):\n",
        "        tags_list.append('[PAD]')\n",
        "    return tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to find the Named Entity Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nerTagging(document):\n",
        "    NE_Tag_table = []\n",
        "    tokens = []\n",
        "    # loading pre-trained model of NER\n",
        "    entity_tagging_model = en_core_web_sm.load()\n",
        "    article = entity_tagging_model(document)\n",
        "    sentences = [x for x in article.sents]\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            NE_Tag_table.append(str(word.ent_type_))\n",
        "            tokens.append(str(word).lower())\n",
        "    for i in range(len(NE_Tag_table)):\n",
        "        if(NE_Tag_table[i] == ''):\n",
        "            NE_Tag_table[i] = \"O\"\n",
        "\n",
        "    return tokens, NE_Tag_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to get the wordnet POS tag and convert to use with lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getWordnetPos(tags):\n",
        "    if tags.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tags.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tags.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif tags.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to Lemmattize the words using the POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatization(tokens, tags):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmitized = [lemmatizer.lemmatize(tokens[ind], pos=getWordnetPos(tags[ind])) for ind in range(len(tokens))]  \n",
        "    return lemmitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Function to Preprocess the Questions list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [],
      "source": [
        "def questionPreprocess(question):\n",
        "    question_tokens = tokenizeList(question)\n",
        "    question_tokens_padded = pad_sequences(question_tokens)\n",
        "    embedded_question_list = word2VecDocuments(question_tokens_padded)\n",
        "    question_batch_torch = torch.from_numpy(np.array(embedded_question_list)).float().to(device)\n",
        "    return question_batch_torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Helper functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {},
      "outputs": [],
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2.QA Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tokenising the Question List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "question_list = formatted_training_data[\"Question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_batch_torch = questionPreprocess(question_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_batch_torch = question_batch_torch[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The length of each sequence\n",
        "seq_length = question_batch_torch.shape[1]\n",
        "# The input feature dimension\n",
        "n_input = question_batch_torch.shape[2]\n",
        "\n",
        "# Set the hyperparameters \n",
        "n_hidden = 151\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bi_RNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_RNN_Model, self).__init__()\n",
        "        # set the bidirectional to True\n",
        "        self.rnn = nn.RNN(n_input, n_hidden, batch_first =True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        x, h_n = self.rnn(x)\n",
        "        # concat the last hidden state from two direction\n",
        "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
        "        return hidden_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Document Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Tokenising the document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_list = formatted_training_data[\"Document\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### NER Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_tokens = []\n",
        "NER_tags = []\n",
        "\n",
        "for document in document_list:\n",
        "    tokens, tags = nerTagging(document)\n",
        "    document_tokens.append(tokens)\n",
        "    NER_tags.append(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Padding the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_tokens_padded = pad_sequences(document_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Adding [PAD] to NER Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ind in range(len(document_tokens_padded)):\n",
        "    tag_len = len(NER_tags[ind])\n",
        "    total_len = len(document_tokens_padded[ind])\n",
        "    for i in range(tag_len,total_len):\n",
        "        NER_tags[ind].append(\"[PAD]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Getting the Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer_list = formatted_training_data[\"Answer\"]\n",
        "answer_tokens = tokenizeList(answer_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_labels = list()\n",
        "\n",
        "for ind in range(len(document_tokens)):\n",
        "    len_org_document = len(document_tokens[ind])\n",
        "    document_labels.append(generateLabels(document_tokens_padded[ind], answer_tokens[ind],len_org_document))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embedding the Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedded_document_labels = word2VecDocuments(document_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_label_vector_torch = torch.from_numpy(np.array(embedded_document_labels)).float().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_tags = []\n",
        "lem_document_tokens = []\n",
        "tf_idf = []\n",
        "for ind in range(len(document_tokens_padded)):\n",
        "    len_org_document = len(document_tokens[ind])\n",
        "    tags = posTagging(document_tokens_padded[ind],len_org_document)\n",
        "    pos_tags.append(tags)\n",
        "    lem_document_tokens.append(lemmatization(document_tokens_padded[ind],tags))\n",
        "    tf_idf.append(tfIdf(lem_document_tokens[ind],len_org_document))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedded_document_list = word2VecDocuments(lem_document_tokens)\n",
        "embedded_pos_tags = word2VecDocuments(pos_tags)\n",
        "embedded_NER_tags = word2VecDocuments(NER_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_vector = []\n",
        "for i in range(5):\n",
        "    embedded_document = embedded_document_list[i]\n",
        "    embedded_pos = embedded_pos_tags[i]\n",
        "    embedded_NER = embedded_NER_tags[i]\n",
        "    tfidf = tf_idf[i]\n",
        "\n",
        "    \n",
        "    token_vector =  []\n",
        "    for j in range(len(embedded_document)):\n",
        "        vector = []\n",
        "        vector.extend(embedded_document[j])\n",
        "        vector.extend(embedded_pos[j])\n",
        "        vector.extend(embedded_NER[j])\n",
        "        vector.append(tfidf[j])\n",
        "        token_vector.append(np.array(vector))\n",
        "    document_vector.append(token_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DT  : \t a  : \t a  : \t 0.027444818892627048  : \t O\n",
            "RB  : \t partly  : \t partly  : \t 0.006861204723156762  : \t O\n",
            "VBN  : \t submerged  : \t submerge  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t glacier  : \t glacier  : \t 0.04116722833894057  : \t O\n",
            "NN  : \t cave  : \t cave  : \t 0.04802843306209734  : \t O\n",
            "IN  : \t on  : \t on  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t perito  : \t perito  : \t 0.006861204723156762  : \t PERSON\n",
            "NN  : \t moreno  : \t moreno  : \t 0.006861204723156762  : \t PERSON\n",
            "NN  : \t glacier  : \t glacier  : \t 0.04116722833894057  : \t PERSON\n",
            ".  : \t .  : \t .  : \t 0.020583614169470285  : \t O\n",
            "DT  : \t the  : \t the  : \t 0.020583614169470285  : \t O\n",
            "NN  : \t ice  : \t ice  : \t 0.034306023615783804  : \t O\n",
            "NN  : \t facade  : \t facade  : \t 0.006861204723156762  : \t O\n",
            "VBZ  : \t is  : \t be  : \t 0.027444818892627048  : \t O\n",
            "RB  : \t approximately  : \t approximately  : \t 0.006861204723156762  : \t O\n",
            "CD  : \t 60  : \t 60  : \t 0.006861204723156762  : \t O\n",
            "NNS  : \t m  : \t m  : \t 0.006861204723156762  : \t O\n",
            "JJ  : \t high  : \t high  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t ice  : \t ice  : \t 0.034306023615783804  : \t O\n",
            "NNS  : \t formations  : \t formation  : \t 0.006861204723156762  : \t O\n",
            "IN  : \t in  : \t in  : \t 0.006861204723156762  : \t O\n",
            "DT  : \t the  : \t the  : \t 0.020583614169470285  : \t O\n",
            "NN  : \t titlis  : \t titlis  : \t 0.006861204723156762  : \t NORP\n",
            "NN  : \t glacier  : \t glacier  : \t 0.04116722833894057  : \t O\n",
            "VBP  : \t cave  : \t cave  : \t 0.04802843306209734  : \t O\n",
            "DT  : \t a  : \t a  : \t 0.027444818892627048  : \t O\n",
            "NN  : \t glacier  : \t glacier  : \t 0.04116722833894057  : \t O\n",
            "NN  : \t cave  : \t cave  : \t 0.04802843306209734  : \t O\n",
            "VBZ  : \t is  : \t be  : \t 0.027444818892627048  : \t O\n",
            "DT  : \t a  : \t a  : \t 0.027444818892627048  : \t O\n",
            "NN  : \t cave  : \t cave  : \t 0.04802843306209734  : \t O\n",
            "VBN  : \t formed  : \t form  : \t 0.006861204723156762  : \t O\n",
            "IN  : \t within  : \t within  : \t 0.006861204723156762  : \t O\n",
            "DT  : \t the  : \t the  : \t 0.020583614169470285  : \t O\n",
            "NN  : \t ice  : \t ice  : \t 0.034306023615783804  : \t O\n",
            "IN  : \t of  : \t of  : \t 0.006861204723156762  : \t O\n",
            "DT  : \t a  : \t a  : \t 0.027444818892627048  : \t O\n",
            "NN  : \t glacier  : \t glacier  : \t 0.04116722833894057  : \t O\n",
            ".  : \t .  : \t .  : \t 0.020583614169470285  : \t O\n",
            "NN  : \t glacier  : \t glacier  : \t 0.04116722833894057  : \t O\n",
            "NNS  : \t caves  : \t cave  : \t 0.04802843306209734  : \t O\n",
            "VBP  : \t are  : \t be  : \t 0.027444818892627048  : \t O\n",
            "RB  : \t often  : \t often  : \t 0.006861204723156762  : \t O\n",
            "VBN  : \t called  : \t call  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t ice  : \t ice  : \t 0.034306023615783804  : \t O\n",
            "NNS  : \t caves  : \t cave  : \t 0.04802843306209734  : \t O\n",
            ",  : \t ,  : \t ,  : \t 0.006861204723156762  : \t O\n",
            "CC  : \t but  : \t but  : \t 0.006861204723156762  : \t O\n",
            "DT  : \t this  : \t this  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t term  : \t term  : \t 0.006861204723156762  : \t O\n",
            "VBZ  : \t is  : \t be  : \t 0.027444818892627048  : \t O\n",
            "RB  : \t properly  : \t properly  : \t 0.006861204723156762  : \t O\n",
            "VBN  : \t used  : \t use  : \t 0.006861204723156762  : \t O\n",
            "TO  : \t to  : \t to  : \t 0.006861204723156762  : \t O\n",
            "VB  : \t describe  : \t describe  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t bedrock  : \t bedrock  : \t 0.006861204723156762  : \t O\n",
            "NNS  : \t caves  : \t cave  : \t 0.04802843306209734  : \t O\n",
            "WDT  : \t that  : \t that  : \t 0.006861204723156762  : \t O\n",
            "VBP  : \t contain  : \t contain  : \t 0.006861204723156762  : \t O\n",
            "NN  : \t year  : \t year  : \t 0.006861204723156762  : \t DATE\n",
            ":  : \t -  : \t -  : \t 0.006861204723156762  : \t DATE\n",
            "NN  : \t round  : \t round  : \t 0.006861204723156762  : \t DATE\n",
            "NN  : \t ice  : \t ice  : \t 0.034306023615783804  : \t O\n",
            ".  : \t .  : \t .  : \t 0.020583614169470285  : \t O\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n",
            "[PAD]  : \t [PAD]  : \t [PAD]  : \t 0  : \t [PAD]\n"
          ]
        }
      ],
      "source": [
        "# tags = pos_tags[0]\n",
        "# padded = document_tokens_padded[0]\n",
        "# lemm = lem_document_tokens[0]\n",
        "# tfidf = tf_idf[0]\n",
        "# NERTags = NER_tags[0]\n",
        "# for i in range(len(tags)):\n",
        "#     print(tags[i],\" : \\t\",padded[i], \" : \\t\", lemm[i], \" : \\t\", tfidf[i], \" : \\t\", NERTags[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert input into tensors and set them to GPU by using tensor.to(device)\n",
        "input_document_vector_torch = torch.from_numpy(np.array(document_vector)).float().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we simply use the maximum sentence length \n",
        "MAX_LENGTH = max([len(s) for s in document_tokens_padded])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Document Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_document_vector_torch = input_document_vector_torch[0:5]\n",
        "target_label_vector_torch = target_label_vector_torch[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attention_output = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    # We will practise the scaled dot product attention in the exercise\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\" \n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        \n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, 2*self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*4, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, question_summary, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, question_summary.T.unsqueeze(0)),dim=-1) \n",
        "            #print(attn_weights)\n",
        "            attn_output = torch.bmm(attn_weights, question_summary.unsqueeze(0))\n",
        "            #print(attn_output.shape)\n",
        "            attention_output = attn_output\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, question_summary):\n",
        "        \n",
        "        _, hidden = self.gru(input, hidden)\n",
        "\n",
        "        concat_output = self.cal_attention(hidden, question_summary, AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(input_question_tensor, input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    \n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "\n",
        "    # it is for storing the hidden states of input sequence later, which will be used for calculating the attention during the decoding process\n",
        "    encoder_hiddens = torch.zeros(1, 1060, 302, device=device)\n",
        "\n",
        "    # zero-initialize an initial hidden state \n",
        "    #encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    loss = 0    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # print(input_tensor[10])\n",
        "\n",
        "    # Feed the input_tensor into the encoder we defined\n",
        "    # for ind in range(0,question_batch_torch.shape[0],batch_size):\n",
        "    encoder.train()\n",
        "    question_summary = encoder(input_question_tensor)\n",
        "\n",
        "    # Use the <BOS> as the first token into decoder for generation\n",
        "    decoder_input = input_tensor\n",
        "\n",
        "    # decoder_hidden = n_hidden \n",
        "\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for i in range(target_length):\n",
        "        #print(decoder_input[i])\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, encoder_hiddens, question_summary)\n",
        "        target = target_tensor[i]\n",
        "        for j in range(len(target_tensor[i])):\n",
        "            loss += criterion(decoder_output[j], target[j]) \n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "def trainIters(encoder, decoder, n_iters, print_every=200, plot_every=200, learning_rate=0.002):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    #criterion = nn.NLLLoss()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        input_question_tensor = question_batch_torch\n",
        "        input_tensor = input_document_vector_torch\n",
        "        target_tensor = target_label_vector_torch\n",
        "\n",
        "        loss = train(input_question_tensor, input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1060, 302])\n",
            "torch.Size([1, 1060, 302])\n",
            "torch.Size([1, 1060, 302])\n",
            "torch.Size([1, 1060, 302])\n",
            "torch.Size([1, 1060, 302])\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 151\n",
        "embedding = nn.Embedding(len(target_label_vector_torch), hidden_size)\n",
        "encoder1 = Bi_RNN_Model().to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, 50, embedding, dropout_p=0.1).to(device)\n",
        "\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 1, print_every=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, question, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_question = question\n",
        "\n",
        "        input_length = input_question.size()[0]\n",
        "        encoder_hidden = n_hidden\n",
        "\n",
        "        encoder_hiddens = torch.zeros(1,max_length, 2*encoder_hidden, device=device)\n",
        "\n",
        "\n",
        "        encoder_hidden = encoder(input_question)\n",
        "        encoder_hiddens += encoder_hidden[0, 0]\n",
        "\n",
        "        decoder_input = input_document_vector_torch\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, encoder_hiddens, encoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1) # simply adopt the predicted tag with the highest probabiity\n",
        "            print(len(topi))\n",
        "\n",
        "            decoded_words.append(document_vector[topi]) # get the predicted word based on the index\n",
        "            # use the predicted output as the input for the next time step generation\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {},
      "outputs": [],
      "source": [
        "questionlist = question_list[0:5]\n",
        "preprocessed_question = questionPreprocess(questionlist)\n",
        "question1 = preprocessed_question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1060\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "only integer tensors of a single element can be converted to an index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem 4/CITS4012/Project/main/Final.ipynb Cell 101'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem%204/CITS4012/Project/main/Final.ipynb#ch0000136?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(evaluate(encoder1, attn_decoder1, question1, max_length\u001b[39m=\u001b[39;49mMAX_LENGTH))\n",
            "\u001b[1;32m/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem 4/CITS4012/Project/main/Final.ipynb Cell 99'\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, question, max_length)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem%204/CITS4012/Project/main/Final.ipynb#ch0000135?line=21'>22</a>\u001b[0m topv, topi \u001b[39m=\u001b[39m decoder_output\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtopk(\u001b[39m1\u001b[39m) \u001b[39m# simply adopt the predicted tag with the highest probabiity\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem%204/CITS4012/Project/main/Final.ipynb#ch0000135?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(topi))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem%204/CITS4012/Project/main/Final.ipynb#ch0000135?line=24'>25</a>\u001b[0m decoded_words\u001b[39m.\u001b[39mappend(document_vector[topi]) \u001b[39m# get the predicted word based on the index\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem%204/CITS4012/Project/main/Final.ipynb#ch0000135?line=25'>26</a>\u001b[0m \u001b[39m# use the predicted output as the input for the next time step generation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naufaln/Library/CloudStorage/OneDrive-Personal/UWA/Sem%204/CITS4012/Project/main/Final.ipynb#ch0000135?line=26'>27</a>\u001b[0m decoder_input \u001b[39m=\u001b[39m topi\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdetach()\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ],
      "source": [
        "# print(evaluate(encoder1, attn_decoder1, question1, max_length=MAX_LENGTH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Model Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNGfO0h9I3W"
      },
      "source": [
        "###3.1. Input Embedding Ablation Study\n",
        "\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVsyvrc9VHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX7nFwMo9WBE"
      },
      "source": [
        "###3.2. Attention Ablation Study\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfRK-BeiNSVi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0VAR8GF9hSD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzGjUe6NDnB"
      },
      "source": [
        "###3.3. Hyper Parameter Testing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xj4PNyrNDBH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
